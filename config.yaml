experiment:
  name: "gnn_test" # Adjusted for GNN experiment
  base_path: "/home/kowrisaan/jathu/cost_model_LSTM"

data_generation:
  train_dataset_file: "/home/kowrisaan/jathu/cost_model_LSTM/data/train_data_sample_500-programs_60k-schedules.pkl"
  valid_dataset_file: "/home/kowrisaan/jathu/cost_model_LSTM/data/val_data_sample_125-programs_20k-schedules.pkl"
  benchmark_dataset_file: "/home/kowrisaan/jathu/cost_model_LSTM/data"
  batch_size: 2048
  nb_processes: 8
  min_functions_per_tree_footprint: 2

training:
  log_file: "gnn_logs.txt"
  lr: 0.001
  max_epochs: 100
  training_gpu: "cuda"
  validation_gpu: "cuda"
  continue_training: False
  model_weights_path: "/home/kowrisaan/jathu/cost_model_LSTM/weights"

testing:
  testing_model_weights_path: "/home/kowrisaan/jathu/cost_model_LSTM/weights/best_model_gnn_test.pt"
  gpu: "cpu"

wandb:
  use_wandb: False
  project: "gnn_release_model"

model:
  input_size: 846 # Size of the input. Here we specify the size of the computation vector.
  comp_embed_layer_sizes:
    - 600
    - 350
    - 200
    - 180

  dropout_prob: 0.05
  drops: # Dropout layers probabilities
    - 0.050
    - 0.050
    - 0.050
    - 0.050
    - 0.050

defaults:
  - override hydra/job_logging: disabled
